<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  
  <!-- Favicon code from realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#8b51a3">
<meta name="msapplication-TileColor" content="#563d7c">
<meta name="theme-color" content="#ffffff">

  <!--jQuery-->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

  <!-- Fonts & Icons -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
  <link href='//spoqa.github.io/spoqa-han-sans/css/SpoqaHanSans-kr.css' rel='stylesheet' type='text/css'>

  <!-- CSS -->
  <link rel="stylesheet" href="/assets/css/main.css">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Publications | Grape Academic Theme</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Publications" />
<meta name="author" content="Luis Armando Pérez Rey" />
<meta property="og:locale" content="en_GB" />
<meta name="description" content="List of publications" />
<meta property="og:description" content="List of publications" />
<meta property="og:site_name" content="Grape Academic Theme" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Publications" />
<meta name="twitter:site" content="@Luis Armando Pérez Rey" />
<meta name="twitter:creator" content="@Luis Armando Pérez Rey" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Luis Armando Pérez Rey"},"description":"List of publications","headline":"Publications","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"/assets/img/smile.png"},"name":"Luis Armando Pérez Rey"},"url":"/publications.html"}</script>
<!-- End Jekyll SEO tag -->

</head>
<!--jQuery-->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
<body>
  <div class="container">
    

<section id="header-nav">
  <header>
    <nav>
      <ul>
        
        <a href="/">
          <li class="btn-nav">Home</li>
        </a>
        <li class="current">Publications</li>
        
          <a href="/presentations">
            <li class="btn-nav">Presentations</li>
          </a>
        
        
          <a href="/blog">
            <li class="btn-nav">Blog</li>
          </a>
          <a href="/tags">
            <li class="btn-nav">Tags</li>
          </a>
        
        <!-- presentations -->
        
      </ul>
    </nav>
  </header>
</section>
<!--publications.js-->
<script type="text/javascript" src="/assets/js/publications.js"></script>
<div id="publications">
  <section class="bg"></section>
  <h1 class="title">Publications</h1>
  <h2 class="bibliography">2022</h2>
<ol class="bibliography"><li><!-- Entry bib key -->
<div id="PerezRey2022a">
  
  <!-- Title -->
  <div class="title">Equivariant Representations for Non-Free Group Actions</div>
  <!-- Author -->
  <div class="author">Perez Rey, Luis Armando,&nbsp;Marchetti, Giovanni Luca,&nbsp;Kragic, Danica,&nbsp;Jarnikov, Dmitri,&nbsp;and Holenderski, Mike
  </div>

  <!-- Journal/Book title and date -->
  
  <div class="third-line">
    In NeurIPS 2022 Workshop on Symmetry and Geometry in Neural Representations, pp. 1–8, 2022
  </div>

  <!-- Links/Buttons -->
  <div class="links">
  </div>

  
</div></li>
<li><!-- Entry bib key -->
<div id="PerezRey2022">
  
  <!-- Title -->
  <div class="title">Identifying the Sources of Epistemic Uncertainty in Image-Based Object Classification</div>
  <!-- Author -->
  <div class="author">Perez Rey, Luis A.,&nbsp;Isler, Berk,&nbsp;Holenderski, Mike,&nbsp;and Jarnikov, Dmitri
  </div>

  <!-- Journal/Book title and date -->
  
  <div class="third-line">
    2022
  </div>

  <!-- Links/Buttons -->
  <div class="links">
  </div>

  
</div></li></ol>
<h2 class="bibliography">2021</h2>
<ol class="bibliography"><li><!-- Entry bib key -->
<div id="Sipiran2021">
  
  <!-- Title -->
  <div class="title">SHREC 2021: Retrieval of Cultural Heritage Objects</div>
  <!-- Author -->
  <div class="author">Sipiran, Ivan,&nbsp;Lazo, Patrick,&nbsp;Lopez, Cristian,&nbsp;Jimenez, Milagritos,&nbsp;Bagewadi, Nihar,&nbsp;Bustos, Benjamin,&nbsp;Dao, Hieu,&nbsp;Gangisetty, Shankar,&nbsp;Hanik, Martin,&nbsp;Ho-Thi, Ngoc Phuong,&nbsp;Holenderski, Mike,&nbsp;Jarnikov, Dmitri,&nbsp;Labrada, Arniel,&nbsp;Lengauer, Stefan,&nbsp;Licandro, Roxane,&nbsp;Nguyen, Dinh Huan,&nbsp;Nguyen-Ho, Thang Long,&nbsp;Perez Rey, Luis A.,&nbsp;Pham, Bang Dang,&nbsp;Pham, Minh Khoi,&nbsp;Preiner, Reinhold,&nbsp;Schreck, Tobias,&nbsp;Trinh, Quoc Huy,&nbsp;Tonnaer, Loek,&nbsp;Tycowicz, Christoph,&nbsp;and Vu-Le, The Anh
  </div>

  <!-- Journal/Book title and date -->
  
  <div class="third-line">
    Computers and Graphics, vol. 100, pp. 1–20, 2021
  </div>

  <!-- Links/Buttons -->
  <div class="links">
      [<a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>]
      [<a href="https://doi.org/10.1016/j.cag.2021.07.010" class="btn btn-sm z-depth-0" role="button">URL</a>]
      [<a href="http://doi.org/10.1016/j.cag.2021.07.010" class="btn btn-sm z-depth-0" role="button">DOI</a>]
  </div>

  <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>This paper presents the methods and results of the SHREC’21 track on a dataset of cultural heritage (CH) objects. We present a dataset of 938 scanned models that have varied geometry and artistic styles. For the competition, we propose two challenges: the retrieval-by-shape challenge and the retrieval-by-culture challenge. The former aims at evaluating the ability of retrieval methods to discriminate cultural heritage objects by overall shape. The latter focuses on assessing the effectiveness of retrieving objects from the same culture. Both challenges constitute a suitable scenario to evaluate modern shape retrieval methods in a CH domain. Ten groups participated in the challenges: thirty runs were submitted for the retrieval-by-shape task, and twenty-six runs were submitted for the retrieval-by-culture task. The results show a predominance of learning methods on image-based multi-view representations to characterize 3D objects. Nevertheless, the problem presented in our challenges is far from being solved. We also identify the potential paths for further improvements and give insights into the future directions of research.</p>
    </div>
</div></li>
<li><!-- Entry bib key -->
<div id="Perez2021">
  
  <!-- Title -->
  <div class="title">Content-Based Image Retrieval from Weakly-Supervised Disentangled Representations</div>
  <!-- Author -->
  <div class="author">Pérez Rey, Luis A.,&nbsp;Holenderski, Mike,&nbsp;and Jarnikov, Dmitri
  </div>

  <!-- Journal/Book title and date -->
  
  <div class="third-line">
    In NeurIPS 2021 Deep Generative Models and Downstream Applications Workshop, 2021
  </div>

  <!-- Links/Buttons -->
  <div class="links">
      [<a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>]
  </div>

  <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>In content-based image retrieval (CBIR), a database of images is ordered based on the similarity to a query image. Similarity criterion is usually determined with respect to a shared category e.g. whether the database images contain an object of the same type as depicted in the query. Depending on the situation, multiple similarity criteria can be relevant such as the type of object, its color, or the depicted background. Ideally, a dataset labeled with all possible criteria information is available for training a model for computing the similarity. Typically, this is not the case. In this paper, we explore the use of disentangled representations for CBIR with respect to multiple criteria. To alleviate the need for labels, the models used to create the representations are learned via weak supervision by using data organized into groups with shared information. We show that such models can attain better retrieval performances compared to unsupervised baselines.</p>
    </div>
</div></li></ol>
<h2 class="bibliography">2020</h2>
<ol class="bibliography"><li><!-- Entry bib key -->
<div id="Tonnaer2020a">
  
  <!-- Title -->
  <div class="title">Quantifying and Learning Linear Symmetry-Based Disentanglement</div>
  <!-- Author -->
  <div class="author">Tonnaer, Loek,&nbsp;Rey, Luis A. Pérez,&nbsp;Menkovski, Vlado,&nbsp;Holenderski, Mike,&nbsp;and Portegies, Jacobus W.
  </div>

  <!-- Journal/Book title and date -->
  
  <div class="third-line">
    2020
  </div>

  <!-- Links/Buttons -->
  <div class="links">
      [<a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>]
      [<a href="http://arxiv.org/abs/2011.06070" class="btn btn-sm z-depth-0" role="button">URL</a>]
  </div>

  <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>The definition of Linear Symmetry-Based Disentanglement (LSBD) formalizes the notion of linearly disentangled representations, but there is currently no metric to quantify LSBD. Such a metric is crucial to evaluate LSBD methods and to compare to previous understandings of disentanglement. We propose }\backslashmathcal{D}_\backslashmathrm{LSBD}}, a mathematically sound metric to quantify LSBD, and provide a practical implementation for }\backslashmathrm{SO}(2)} groups. Furthermore, from this metric we derive LSBD-VAE, a semi-supervised method to learn LSBD representations. We demonstrate the utility of our metric by showing that (1) common VAE-based disentanglement methods don’t learn LSBD representations, (2) LSBD-VAE as well as other recent methods can learn LSBD representations, needing only limited supervision on transformations, and (3) various desirable properties expressed by existing disentanglement metrics are also achieved by LSBD representations.</p>
    </div>
</div></li>
<li><!-- Entry bib key -->
<div id="Rey2020">
  
  <!-- Title -->
  <div class="title">A Metric for Linear Symmetry-Based Disentanglement</div>
  <!-- Author -->
  <div class="author">Pérez Rey, Luis A.,&nbsp;Tonnaer, Loek,&nbsp;Menkovski, Vlado,&nbsp;Holenderski, Mike,&nbsp;and Portegies, Jacobus W.
  </div>

  <!-- Journal/Book title and date -->
  
  <div class="third-line">
    In NeurIPS 2020 Workshop on Differential Geometry meets Deep Learning, 2020
  </div>

  <!-- Links/Buttons -->
  <div class="links">
      [<a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>]
      [<a href="http://arxiv.org/abs/2011.13306" class="btn btn-sm z-depth-0" role="button">URL</a>]
  </div>

  <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>The definition of Linear Symmetry-Based Disentanglement (LSBD) proposed by (Higgins et al., 2018) outlines the properties that should characterize a disentangled representation that captures the symmetries of data. However, it is not clear how to measure the degree to which a data representation fulfills these properties. We propose a metric for the evaluation of the level of LSBD that a data representation achieves. We provide a practical method to evaluate this metric and use it to evaluate the disentanglement of the data representations obtained for three datasets with underlying }SO(2)} symmetries.</p>
    </div>
</div></li>
<li><!-- Entry bib key -->
<div id="Tonnaer2020">
  
  <!-- Title -->
  <div class="title">Quantifying and Learning Disentangled Representations with Limited Supervision</div>
  <!-- Author -->
  <div class="author">Tonnaer, Loek,&nbsp;Rey, Luis A. Pérez,&nbsp;Menkovski, Vlado,&nbsp;Holenderski, Mike,&nbsp;and Portegies, Jacobus W.
  </div>

  <!-- Journal/Book title and date -->
  
  <div class="third-line">
    pp. 1–15, 2020
  </div>

  <!-- Links/Buttons -->
  <div class="links">
      [<a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>]
      [<a href="http://arxiv.org/abs/2011.06070" class="btn btn-sm z-depth-0" role="button">URL</a>]
  </div>

  <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Learning low-dimensional representations that disentangle the underlying factors of variation in data has been posited as an important step towards interpretable machine learning with good generalization. To address the fact that there is no consensus on what disentanglement entails, Higgins et al. (2018) propose a formal definition for Linear Symmetry-Based Disentanglement, or LSBD, arguing that underlying real-world transformations give exploitable structure to data. Although several works focus on learning LSBD representations, such methods require supervision on the underlying transformations for the entire dataset, and cannot deal with unlabeled data. Moreover, none of these works provide a metric to quantify LSBD. We propose a metric to quantify LSBD representations that is easy to compute under certain well-defined assumptions. Furthermore, we present a method that can leverage unlabeled data, such that LSBD representations can be learned with limited supervision on transformations. Using our LSBD metric, our results show that limited supervision is indeed sufficient to learn LSBD representations.</p>
    </div>
</div></li>
<li><!-- Entry bib key -->
<div id="PerezRey2020">
  
  <!-- Title -->
  <div class="title">Diffusion Variational Autoencoders</div>
  <!-- Author -->
  <div class="author">Perez Rey, Luis A.,&nbsp;Menkovski, Vlado,&nbsp;and Portegies, Jim
  </div>

  <!-- Journal/Book title and date -->
  
  <div class="third-line">
    International Joint Conference on Artificial Intelligence, vol. 2021-Janua, pp. 2704–2710, 2020
  </div>

  <!-- Links/Buttons -->
  <div class="links">
      [<a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>]
      [<a href="http://doi.org/10.24963/ijcai.2020/375" class="btn btn-sm z-depth-0" role="button">DOI</a>]
  </div>

  <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>A standard Variational Autoencoder, with a Euclidean latent space, is structurally incapable of capturing topological properties of certain datasets. To remove topological obstructions, we introduce Diffusion Variational Autoencoders (?VAE) with arbitrary (closed) manifolds as a latent space. A Diffusion Variational Autoencoder uses transition kernels of Brownian motion on the manifold. In particular, it uses properties of the Brownian motion to implement the reparametrization trick and fast approximations to the KL divergence. We show that the ?VAE is indeed capable of capturing topological properties for datasets with a known underlying latent structure derived from generative processes such as rotations and translations.</p>
    </div>
</div></li>
<li><!-- Entry bib key -->
<div id="Yuan2020">
  
  <!-- Title -->
  <div class="title">A Comparison of Methods for 3D Scene Shape Retrieval</div>
  <!-- Author -->
  <div class="author">Yuan, Juefei,&nbsp;Abdul-Rashid, Hameed,&nbsp;Li, Bo,&nbsp;Lu, Yijuan,&nbsp;Schreck, Tobias,&nbsp;Bai, Song,&nbsp;Bai, Xiang,&nbsp;Bui, Ngoc Minh,&nbsp;Do, Minh N.,&nbsp;Do, Trong Le,&nbsp;Duong, Anh Duc,&nbsp;He, Kai,&nbsp;He, Xinwei,&nbsp;Holenderski, Mike,&nbsp;Jarnikov, Dmitri,&nbsp;Le, Tu Khiem,&nbsp;Li, Wenhui,&nbsp;Liu, Anan,&nbsp;Liu, Xiaolong,&nbsp;Menkovski, Vlado,&nbsp;Nguyen, Khac Tuan,&nbsp;Nguyen, Thanh An,&nbsp;Nguyen, Vinh Tiep,&nbsp;Nie, Weizhi,&nbsp;Ninh, Van Tu,&nbsp;Rey, Perez,&nbsp;Su, Yuting,&nbsp;Ton-That, Vinh,&nbsp;Tran, Minh Triet,&nbsp;Wang, Tianyang,&nbsp;Xiang, Shu,&nbsp;Zhe, Shandian,&nbsp;Zhou, Heyu,&nbsp;Zhou, Yang,&nbsp;and Zhou, Zhichao
  </div>

  <!-- Journal/Book title and date -->
  
  <div class="third-line">
    Computer Vision and Image Understanding, vol. 201, 2020
  </div>

  <!-- Links/Buttons -->
  <div class="links">
      [<a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>]
      [<a href="http://doi.org/10.1016/j.cviu.2020.103070" class="btn btn-sm z-depth-0" role="button">DOI</a>]
  </div>

  <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>3D scene shape retrieval is a brand new but important research direction in content-based 3D shape retrieval. To promote this research area, two Shape Retrieval Contest (SHREC) tracks on 2D scene sketch-based and image-based 3D scene model retrieval have been organized by us in 2018 and 2019, respectively. In 2018, we built the first benchmark for each track which contains 2D and 3D scene data for ten (10) categories, while they share the same 3D scene target dataset. Four and five distinct 3D scene shape retrieval methods have competed with each other in these two contests, respectively. In 2019, to measure and compare the scalability performance of the participating and other promising Query-by-Sketch or Query-by-Image 3D scene shape retrieval methods, we built a much larger extended benchmark for each type of retrieval which has thirty (30) classes and organized two extended tracks. Again, two and three different 3D scene shape retrieval methods have contended in these two tracks, separately. To solicit state-of-the-art approaches, we perform a comprehensive comparison of all the above methods and an additional new retrieval methods by evaluating them on the two benchmarks. The benchmarks, evaluation results and tools are publicly available at our track websites (Yuan et al., 2019 [1]; Abdul-Rashid et al., 2019 [2]; Yuan et al., 2019 [3]; Abdul-Rashid et al., 2019 [4]), while code for the evaluated methods are also available: http://github.com/3DSceneRetrieval.</p>
    </div>
</div></li>
<li><!-- Entry bib key -->
<div id="Yuan2020a">
  
  <!-- Title -->
  <div class="title">A Comparison of Methods for 3D Scene Shape Retrieval</div>
  <!-- Author -->
  <div class="author">Yuan, Juefei,&nbsp;Abdul-Rashid, Hameed,&nbsp;Li, Bo,&nbsp;Lu, Yijuan,&nbsp;Schreck, Tobias,&nbsp;Bai, Song,&nbsp;Bai, Xiang,&nbsp;Bui, Ngoc-Minh,&nbsp;Do, Minh N.,&nbsp;Do, Trong-Le,&nbsp;Duong, Anh-Duc,&nbsp;He, Kai,&nbsp;He, Xinwei,&nbsp;Holenderski, Mike,&nbsp;Jarnikov, Dmitri,&nbsp;Le, Tu-Khiem,&nbsp;Li, Wenhui,&nbsp;Liu, Anan,&nbsp;Liu, Xiaolong,&nbsp;Menkovski, Vlado,&nbsp;Nguyen, Khac-Tuan,&nbsp;Nguyen, Thanh-An,&nbsp;Nguyen, Vinh-Tiep,&nbsp;Nie, Weizhi,&nbsp;Ninh, Van-Tu,&nbsp;Rey, Perez,&nbsp;Su, Yuting,&nbsp;Ton-That, Vinh,&nbsp;Tran, Minh-Triet,&nbsp;Wang, Tianyang,&nbsp;Xiang, Shu,&nbsp;Zhe, Shandian,&nbsp;Zhou, Heyu,&nbsp;Zhou, Yang,&nbsp;and Zhou, Zhichao
  </div>

  <!-- Journal/Book title and date -->
  
  <div class="third-line">
    Computer Vision and Image Understanding, vol. 201, pp. 103070, Dec, 2020
  </div>

  <!-- Links/Buttons -->
  <div class="links">
      [<a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>]
      [<a href="https://linkinghub.elsevier.com/retrieve/pii/S1077314220301090" class="btn btn-sm z-depth-0" role="button">URL</a>]
      [<a href="http://doi.org/10.1016/j.cviu.2020.103070" class="btn btn-sm z-depth-0" role="button">DOI</a>]
  </div>

  <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>3D scene shape retrieval is a brand new but important research direction in content-based 3D shape retrieval. To promote this research area, two Shape Retrieval Contest (SHREC) tracks on 2D scene sketch-based and image-based 3D scene model retrieval have been organized by us in 2018 and 2019, respectively. In 2018, we built the first benchmark for each track which contains 2D and 3D scene data for ten (10) categories, while they share the same 3D scene target dataset. Four and five distinct 3D scene shape retrieval methods have competed with each other in these two contests, respectively. In 2019, to measure and compare the scalability performance of the participating and other promising Query-by-Sketch or Query-by-Image 3D scene shape retrieval methods, we built a much larger extended benchmark for each type of retrieval which has thirty (30) classes and organized two extended tracks. Again, two and three different 3D scene shape retrieval methods have contended in these two tracks, separately. To solicit state-of-the-art approaches, we perform a comprehensive comparison of all the above methods and an additional new retrieval methods by evaluating them on the two benchmarks. The benchmarks, evaluation results and tools are publicly available at our track websites (Yuan et al., 2019 [1]; Abdul-Rashid et al., 2019 [2]; Yuan et al., 2019 [3]; Abdul-Rashid et al., 2019 [4]), while code for the evaluated methods are also available: http://github.com/3DSceneRetrieval.</p>
    </div>
</div></li></ol>
<h2 class="bibliography">2019</h2>
<ol class="bibliography"><li><!-- Entry bib key -->
<div id="Rey2019">
  
  <!-- Title -->
  <div class="title">Can VAEs Capture Topological Properties?</div>
  <!-- Author -->
  <div class="author">Pérez Rey, Luis A.,&nbsp;Menkovski, Vlado,&nbsp;and Portegies, Jacobus W.
  </div>

  <!-- Journal/Book title and date -->
  
  <div class="third-line">
    In , 2019
  </div>

  <!-- Links/Buttons -->
  <div class="links">
      [<a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>]
  </div>

  <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>To what extent can Variational Autoencoders (VAEs) identify semantically meaningful latent variables? Can they at least capture the correct topology if ground-truth latent variables are known? To investigate these questions, we introduce the Diffusion VAE, which allows for arbitrary (closed) manifolds in latent space. A Diffusion VAE uses transition kernels of Brownian motion on the manifold. In particular, it uses properties of the Brownian motion to implement the reparametrization trick and fast approximations to the KL divergence. We show that the Diffusion Variational Autoencoder is indeed capable of capturing topological properties.</p>
    </div>
</div></li></ol>
<h2 class="bibliography">2018</h2>
<ol class="bibliography"><li><!-- Entry bib key -->
<div id="Alzate-Carvajal2018">
  
  <!-- Title -->
  <div class="title">One-Step Nondestructive Functionalization of Graphene Oxide Paper with Amines</div>
  <!-- Author -->
  <div class="author">Alzate-Carvajal, Natalia,&nbsp;Acevedo-Guzmán, Diego A.,&nbsp;Meza-Laguna, Victor,&nbsp;Farías, Mario H.,&nbsp;Pérez-Rey, Luis A.,&nbsp;Abarca-Morales, Edgar,&nbsp;García-Ramírez, Victor A.,&nbsp;Basiuk, Vladimir A.,&nbsp;and Basiuk, Elena V.
  </div>

  <!-- Journal/Book title and date -->
  
  <div class="third-line">
    RSC Advances, vol. 8, pp. 15253–15265, 2018
  </div>

  <!-- Links/Buttons -->
  <div class="links">
      [<a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>]
      [<a href="http://doi.org/10.1039/c8ra00986d" class="btn btn-sm z-depth-0" role="button">DOI</a>]
  </div>

  <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Direct functionalization of prefabricated free-standing graphene oxide paper (GOP) is the only approach suitable for systematic tuning of its mechanical, thermal and electronic characteristics. However, the traditional liquid-phase functionalization can compromise physical integrity of the paper-like material up to its total disintegration. In the present paper, we attempted to apply an alternative, solvent-free strategy for facile and nondestructive functionalization of GOP with 1-octadecylamine (ODA) and 1,12-diaminododecane (DAD) as representatives of aliphatic amines, and with 1-aminopyrene (AP) and 1,5-diaminonaphthalene (DAN) as examples of aromatic amines. The functionalization can be carried out under moderate heating at 150-180 °C for 2 h in vacuum, and proceeds through both amidation and epoxy ring opening reactions. Comparative characterization of pristine and amine-modified GOP samples was carried out by means of Fourier-transform infrared, Raman, and X-ray photoelectron spectroscopy, thermogravimetric and differential thermal analysis, scanning electron and atomic force microscopy. In addition, we compared stability in water, wettability, electrical conductivity and elastic (Young’s) modulus of GOP samples before and after functionalization. The highest content of amine species was obtained in the case of GOP-ODA, followed by GOP-DAD, GOP-AP and GOP-DAN. The functionalization increased mechanical and thermal stability, as well as the electrical conductivity of GOP. The magnitude of each effect depends on the structure of amine employed, which allows for tuning a given GOP characteristic. Morphological characterization showed that, compared to pristine graphene oxide paper, amine-modified mats become relatively ordered layered structures, in which individual GO sheets are organized in a near-parallel fashion.</p>
    </div>
</div></li>
<li><!-- Entry bib key -->
<div id="Rey2018">
  
  <!-- Title -->
  <div class="title">Latent Variable Separation with Variational Autoencoders</div>
  <!-- Author -->
  <div class="author">Perez Rey, Luis A.
  </div>

  <!-- Journal/Book title and date -->
  
  <div class="third-line">
    PhD thesis, Eindhoven University of Technology, 2018
  </div>

  <!-- Links/Buttons -->
  <div class="links">
  </div>

  
</div></li></ol>
<h2 class="bibliography">2015</h2>
<ol class="bibliography"><li><!-- Entry bib key -->
<div id="Basiuk2015">
  
  <!-- Title -->
  <div class="title">Solvent-Free Functionalization of Carbon Nanotube Buckypaper with Amines</div>
  <!-- Author -->
  <div class="author">Basiuk, Elena V.,&nbsp;Ramírez-Calera, Itzel J.,&nbsp;Meza-Laguna, Victor,&nbsp;Abarca-Morales, Edgar,&nbsp;Pérez-Rey, Luis A.,&nbsp;Re, Marilena,&nbsp;Prete, Paola,&nbsp;Lovergine, Nico,&nbsp;Álvarez-Zauco, Edgar,&nbsp;and Basiuk, Vladimir A.
  </div>

  <!-- Journal/Book title and date -->
  
  <div class="third-line">
    Applied Surface Science, vol. 357, pp. 1355–1368, 2015
  </div>

  <!-- Links/Buttons -->
  <div class="links">
      [<a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>]
      [<a href="http://dx.doi.org/10.1016/j.apsusc.2015.09.252" class="btn btn-sm z-depth-0" role="button">URL</a>]
      [<a href="http://doi.org/10.1016/j.apsusc.2015.09.252" class="btn btn-sm z-depth-0" role="button">DOI</a>]
  </div>

  <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>We demonstrate the possibility of fast and efficient solvent-free functionalization of buckypaper (BP) mats prefabricated from oxidized multiwalled carbon nanotubes (MWCNTs-ox), by using three representative amines of different structure: one monofunctional aliphatic amine, octadecylamine (ODA), one monofunctional aromatic amine, 1-aminopyrene (AP), and one aromatic diamine, 1,5-diaminonaphthalene (DAN). The functionalization procedure, which relies on the formation of amide bonds with carboxylic groups of MWCNTs-ox, is performed at 150-180 °C under reduced pressure and takes about 4 h including auxiliary degassing. The amine-treated BP samples (BP-ODA, BP-AP and BP-DAN, respectively) were characterized by means of a variety of analytical techniques such as Fourier-transform infrared and Raman spectroscopy, thermogravimetric and differential thermal analysis, scanning and transmission electron microscopy, scanning helium ion microscopy, and atomic force microscopy. The highest amine content was found for BP-ODA, and the lowest one was observed for BP-DAN, with a possible contribution of non-covalently bonded amine molecules in all three cases. Despite of some differences in spectral and morphological characteristics for amine-functionalized BP samples, they have in common a dramatically increased stability in water as compared to pristine BP and, on the other hand, a relatively invariable electrical conductivity.</p>
    </div>
</div></li></ol>
</div>

<!-- Footer -->
<footer>
  <div class="footer">
    Copyright © 2023
    <a href=""></a>.
    Powered by Jekyll with
    <a href="https://github.com/chrjabs/Grape-Academic-Theme">Grape Academic Theme</a>.
  </div>
</footer>

  </div>
</body>

</html>